---
title: 'Key points of e-prop (WIP)'
date: 2025-06-28
permalink: /posts/2025/06/e-prop/
tags:
  - JAX
  - SNN
  - Tutorial
  - e-prop
---

e-prop is a local learning rule that calculates eligbility traces for every parameter based on only local information. These traces are combines with a learning signal to change weights.

In this post we will figure out what i calculates and why. I'll do a quick explanation of how the rule works, but if you get the general idea you can just skip the next section. And of course, here's the link to the original paper by Bellec et al. [https://www.nature.com/articles/s41467-020-17236-y](https://www.nature.com/articles/s41467-020-17236-y).

## The general idea
WIP

## The actual equations
(TODO neuron equation from bptt) We are going to distinguish between input connections, output connections and recurrent connections. For input connections, let's assume once again assume we only have one recurrent neuron with one input connection, as shown in [Figure 1](#fig:eprop_neuron_diagram).

<figure id="fig:eprop_neuron_diagram">
<style>
.e-prop-diagram {
    display: flex;
    align-items: center;
    justify-content: center;
    gap: 15px;
    margin: 2em auto;
    font-family: monospace;
    font-size: 1.1em;
    width: 100%;
    text-align: center;
}
.neuron-circle {
    width: 60px;
    height: 60px;
    border-radius: 50%;
    border: 2px solid #333;
    display: flex;
    align-items: center;
    justify-content: center;
}
.connection-arrow {
    position: relative;
    width: 80px;
    height: 2px;
    background-color: #333;
}
.connection-arrow::after {
    content: '';
    position: absolute;
    right: -1px;
    top: -4px;
    width: 0;
    height: 0;
    border-top: 5px solid transparent;
    border-bottom: 5px solid transparent;
    border-left: 10px solid #333;
}
.connection-label {
    position: absolute;
    width: 100%;
    text-align: center;
    top: -2.5em;
    font-style: italic;
}
</style>
<div class="e-prop-diagram">
    <span style="color: purple; font-weight: bold;">X[t]</span>
    <div class="connection-arrow">
        <div class="connection-label">$$\textcolor{red}{W_{in}}$$</div>
    </div>
    <div class="neuron-circle" style="color: ForestGreen; font-weight: bold;">U[t]</div>
    <div class="connection-arrow"></div>
    <span style="color: orange; font-weight: bold;">S[t]</span>
</div>
<figcaption><strong>Figure 1:</strong> Simple neuron setup for e-prop analysis with input X[t], membrane potential U[t], input weight W<sub>in</sub>, and output spike S[t].</figcaption>
</figure>


Normally if we were doing backpropagation through time (BPTT), our weight update would be like this

\begin{equation}
\Delta W_{in} = -\eta \frac{\partial E}{\partial W_{in}}
\label{eq:basic_weight_update}
\end{equation}

in other words, we would see how the loss function $$E$$ changes when we change the weight $$W_{in}$$. Then we just have to adjust $$W_in$$ in the direction that decreases the loss. We can expand this term into two different derivatives:

\begin{equation}
\Delta W_{in} = -\eta \sum_{t} \frac{\partial E}{\partial S[t]} \frac{\partial S[t]}{\partial W_{in}}
\label{eq:expanded_weight_update}
\end{equation}

In other words, we see how the loss changes with respect to the outgoing spikes and how these spikes change with respect to the weight. There's a sum because the spikes at every timestep are factored in when using BPTT. Now the key insight of e-prop is that you can compute these two derivatives separately. Let's give them some names:

\begin{equation}
\Delta W_{in} = -\eta \sum_{t} \frac{\partial E}{\partial S[t]} \frac{\partial S[t]}{\partial W_{in}}
\label{eq:eprop_decomposition}
\end{equation}
 

The decomposition in \eqref{eq:eprop_decomposition} leads us to the key insight of e-prop: the right term can we computed locally. We call $\frac{\partial E}{\partial S[t]}$ the learning signal and $\frac{\partial S[t]}{\partial W_{in}}$ the eligibility trace. The eligibility trace is the thing we can compute locally. 

If you have read the backprop tutorial (TODO link), you may remember $$\frac{\partial S[t]}{\partial W_{in}}$$ as $$S'[t]$$. Same thing. We already know its derivative:

\begin{equation}
\textcolor{orange}{S'[t]} = \Theta'(\textcolor{ForestGreen}{U[t]} - \textcolor{brown}{\theta}) \cdot \textcolor{ForestGreen}{U'[t]}
\label{eq:S_t_derivative}
\end{equation}


It required the derivative of $$U'[t]$$ which was simply:

\begin{equation}
\textcolor{ForestGreen}{U'[t]} = \sum_{i=1}^{t} \textcolor{blue}{\alpha}^{t-i} \textcolor{purple}{X[i]}
\label{eq:U_t_derivative}
\end{equation}


and we defined $$\Theta'$$ as the surrogate gradient of the heaviside function. For illustrative purposes we used the derivative of the sigmoid $$\sigma'$$.