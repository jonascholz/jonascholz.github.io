---
title: 'Backpropagation-through-time (BPTT) (WIP)'
date: 2025-06-28
permalink: /posts/2025/06/bptt/
tags:
  - JAX
  - SNN
  - Tutorial
  - bptt
---

BPTT is probably the most commonly used way for training Spiking Neural Networks. Today we will walk through some of the mathematics of the method. By the end of this short article, you will be able to calculate all gradients by hand on paper if you have to. 

## The setup
We will start with the simplest possible version and work our way up. Don't worry, it won't be hard to go from the simple case to the full gradient later. 

Let's assume we have a single leaky integrate-and-fire (LIF) neuron with a single incoming connection $$\textcolor{red}{W_{in}}$$ and there are no recurrent connections, as shown in [Figure 1](#fig:neuron_diagram).

<figure id="fig:neuron_diagram">
<style>
.e-prop-diagram {
    display: flex;
    align-items: center;
    justify-content: center;
    gap: 15px;
    margin: 2em auto;
    font-family: monospace;
    font-size: 1.1em;
    width: 100%;
    text-align: center;
}
.neuron-circle {
    width: 60px;
    height: 60px;
    border-radius: 50%;
    border: 2px solid #333;
    display: flex;
    align-items: center;
    justify-content: center;
}
.connection-arrow {
    position: relative;
    width: 80px;
    height: 2px;
    background-color: #333;
}
.connection-arrow::after {
    content: '';
    position: absolute;
    right: -1px;
    top: -4px;
    width: 0;
    height: 0;
    border-top: 5px solid transparent;
    border-bottom: 5px solid transparent;
    border-left: 10px solid #333;
}
.connection-label {
    position: absolute;
    width: 100%;
    text-align: center;
    top: -2.5em;
    font-style: italic;
}
</style>
<div class="e-prop-diagram">
    <span style="color: purple; font-weight: bold;">X[t]</span>
    <div class="connection-arrow">
        <div class="connection-label">$$\textcolor{red}{W_{in}}$$</div>
    </div>
    <div class="neuron-circle" style="color: ForestGreen; font-weight: bold;">U[t]</div>
    <div class="connection-arrow"></div>
    <span style="color: orange; font-weight: bold;">S[t]</span>
</div>
<figcaption><strong>Figure 1:</strong> Simple LIF neuron setup with input X[t], membrane potential U[t], input weight W<sub>in</sub>, and output spike S[t].</figcaption>
</figure>

In this case we can describe the membrane potential $$\textcolor{ForestGreen}{U[t]}$$ as:

\begin{equation}
\textcolor{ForestGreen}{U[t]} = \textcolor{blue}{\alpha} \textcolor{ForestGreen}{U[t-1]} + \textcolor{red}{W_{in}} \textcolor{purple}{X[t]} - \textcolor{orange}{S[t-1]} \textcolor{brown}{\theta}
\label{eq:lif_full}
\end{equation}

where $$\textcolor{blue}{\alpha}$$ is a decay term, $$\textcolor{purple}{X[t]}$$ is the input at time $$t$$ and $$\textcolor{brown}{\theta}$$ is the firing threshold. $$\textcolor{orange}{S[t-1]}$$ is the outgoing spike at time $$t-1$$. Note that if there was a spike at the previous timestep $$t-1$$ we subtract the firing threshold from the membrane potential in order to reset it. This is a simple version of the LIF refractory mechanism (TODO cite eshraghian). The outgoing spike is defined as 

\begin{equation}
\textcolor{orange}{S[t]} = \Theta(\textcolor{ForestGreen}{U[t]} - \textcolor{brown}{\theta})
\label{eq:spike_function}
\end{equation}

where $$\Theta$$ is the heaviside function. In other words, S[t] is equal to 1 (spike) whenever the membrane potential surpasses threshold and 0 (no spike) otherwise. The heaviside function can be seen in [Figure 2](#fig:surrogate_gradient).

We are going to make one simplification to prevent the math from getting out of control and that is to discard the reset term $$-\textcolor{orange}{S[t]} \textcolor{brown}{\theta}$$. We will add it back at the end, don't worry about it. This is just because the equations get too long. The simplified equation for the neuron's state is now:

\begin{equation}
\textcolor{ForestGreen}{U[t]} = \textcolor{blue}{\alpha} \textcolor{ForestGreen}{U[t-1]} + \textcolor{red}{W_{in}} \textcolor{purple}{X[t]}
\label{eq:lif_simplified}
\end{equation}

We also need to define a loss so we can make meaningful changes with respect to the loss. For simplicity, our loss will simply be the difference between our outgoing spike count $$\textcolor{green}{\hat{y}}$$ and a target spike count $$\textcolor{magenta}{y}$$. Our outgoing spike count $$\textcolor{green}{\hat{y}}$$ is defined as $$\sum_t \textcolor{orange}{S[t]}$$. And the loss is defined as follows

\begin{equation}
\textcolor{olive}{E} = \textcolor{green}{\hat{y}} - \textcolor{magenta}{y}
\label{eq:loss}
\end{equation}


Our goal is to find the weight $$\textcolor{red}{W_{in}}$$ that minimizes the loss $$\textcolor{olive}{E}$$. 

## The gradients

\begin{equation}
\frac{d\textcolor{olive}{E}}{d\textcolor{red}{W_{in}}} = \textcolor{green}{\hat{y}}' - \textcolor{magenta}{y}'
\label{eq:loss_derivative}
\end{equation}


Note that I used $$\textcolor{green}{\hat{y}}'$$ interchangibly with $$\frac{d\textcolor{green}{\hat{y}}}{d\textcolor{red}{W_{in}}}$$ since it's easier to read. But that's just slight of hand, we still need to resolve it to real numbers. First of all, for the ground truth $$\textcolor{magenta}{y}$$ we get $$\textcolor{magenta}{y}' = 0$$, because $$\textcolor{magenta}{y}$$ is just a constant. However, our prediction $$\textcolor{green}{\hat{y}}$$ does depend on $$\textcolor{red}{W_{in}}$$ so let us expand the term to figure out the derivative. Remember that we defined $$\textcolor{green}{\hat{y}} = \sum_t \textcolor{orange}{S[t]}$$. So when we we expand it, it looks like this:


$$\textcolor{green}{\hat{y}} = \textcolor{orange}{S[0]} + \textcolor{orange}{S[1]} + ... \textcolor{orange}{S[T]}$$


$$\textcolor{green}{\hat{y}}' = \textcolor{orange}{S'[0]} + \textcolor{orange}{S'[1]} + ... \textcolor{orange}{S'[T]}$$


Great, so the derivative of y is just the sum of the spike derivaties $$\textcolor{green}{\hat{y}}' = \sum_t \textcolor{orange}{S'[t]}$$. If we knew what $$\textcolor{orange}{S'[t]}$$ is, we could already find the derivative and be done. Recall that by definition $$\textcolor{orange}{S[t]} = \Theta(\textcolor{ForestGreen}{U[t]} - \textcolor{brown}{\theta})$$. What we have here is a function and in it we subtract a constant from another function. The [chain rule](https://en.wikipedia.org/wiki/Differentiation_rules#Chain_rule) tells us how to handle these cases and brings us to:

\begin{equation}
\textcolor{orange}{S'[t]} = \Theta'(\textcolor{ForestGreen}{U[t]} - \textcolor{brown}{\theta}) \cdot (\textcolor{ForestGreen}{U[t]} - \textcolor{brown}{\theta})'
\label{eq:spike_derivative}
\end{equation}


It's the classic outer derivative times inner derivative. Let's start with the inner derivative on the right. In this simple model, $$\textcolor{brown}{\theta}$$ is just a constant and its derivative is $$0$$. Thus


$$(\textcolor{ForestGreen}{U[t]} - \textcolor{brown}{\theta})'$$
$$ = (\textcolor{ForestGreen}{U[t]} - 0)' $$
$$ = \textcolor{ForestGreen}{U'[t]} $$


So the right part is just the derivate of $$U[t]$$ and the full equation simplifies to


$$\textcolor{orange}{S'[t]} = \Theta'(\textcolor{ForestGreen}{U[t]} - \textcolor{brown}{\theta}) \cdot \textcolor{ForestGreen}{U'[t]}$$


We actually have a problem with the left part, because there is no derivative of the heaviside function $$\Theta'$$. 

## Surrogate Gradients

We can estimate the derivative of the heaviside function with the dirac delta function (TODO source), but it's still pretty useless. This is because it's $$0$$ almost everywhere (see figure below), which means this whole equation comes out to $$0$$ most of the time, preventing us from finding any meaningful gradient. 

Instead we use a surrogate gradient function. Intuitively you can think of it like pretending we used a smooth activation function and taking the derivative of that smooth function instead. The trick is that we didn't actually use a smooth activation function for the forward-pass, but it turns out that the gradient is still useful, even if it didn't come from our original function. You can see this trick in [Figure 2](#fig:surrogate_gradient).

<figure id="fig:surrogate_gradient">
<div id="surrogate-gradient-plot" style="width:100%; height:400px;"></div>
<figcaption><strong>Figure 2:</strong> Surrogate gradient concept showing the Heaviside function (left, solid) and its smooth sigmoid approximation (left, dashed), along with their derivatives (right). The Heaviside derivative is the Dirac delta (spike at zero), while the sigmoid derivative provides a smooth, trainable gradient.</figcaption>
</figure>

<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', function() {
    const sigmoidColor = '#1f77b4'; // Muted blue
    const heavisideColor = '#ff7f0e'; // Safety orange

    // Generate data
    const x = [];
    for (let i = -5; i <= 5; i += 0.1) {
        x.push(i);
    }

    // Left plot data
    const y_heaviside = x.map(val => val < 0 ? 0 : 1);
    const y_sigmoid = x.map(val => 1 / (1 + Math.exp(-val)));

    // Right plot data
    const y_sigmoid_deriv = y_sigmoid.map(val => val * (1 - val));

    // Traces for the left plot
    const trace_heaviside = {
        x: x,
        y: y_heaviside,
        mode: 'lines',
        name: 'Heaviside (Θ)',
        line: { color: heavisideColor, width: 3 },
        showlegend: false
    };

    const trace_sigmoid = {
        x: x,
        y: y_sigmoid,
        mode: 'lines',
        name: 'Sigmoid (σ, Surrogate)',
        line: { color: sigmoidColor, dash: 'dash', width: 3 },
        showlegend: false
    };

    // Traces for the right plot
    const trace_dirac = {
        x: [0, 0],
        y: [0, 1], // Representing Dirac as a spike of height 1
        mode: 'lines',
        name: 'Dirac Delta (δ)',
        xaxis: 'x2',
        yaxis: 'y2',
        line: { color: heavisideColor, width: 3 }
    };

    const trace_sigmoid_deriv = {
        x: x,
        y: y_sigmoid_deriv,
        mode: 'lines',
        name: 'Sigmoid Derivative (σ′)',
        xaxis: 'x2',
        yaxis: 'y2',
        line: { color: sigmoidColor, dash: 'dash', width: 3 }
    };

    const layout = {
        title: 'Surrogate Gradient Concept',
        xaxis: {
            domain: [0, 0.45],
            title: 'x',
            zeroline: true
        },
        yaxis: {
            title: 'f(x)',
            range: [-0.1, 1.1]
        },
        xaxis2: {
            domain: [0.55, 1],
            title: 'x',
            zeroline: true
        },
        yaxis2: {
            anchor: 'x2',
            title: 'f\'(x)',
            range: [-0.1, 1.1]
        },
        legend: {
            x: 1.02,
            xanchor: 'left',
            y: 0.5,
            yanchor: 'middle',
            orientation: 'v'
        },
        margin: {
            b: 100,
            r: 150
        }
    };

    Plotly.newPlot('surrogate-gradient-plot', [trace_heaviside, trace_sigmoid, trace_dirac, trace_sigmoid_deriv], layout);
});
</script>

Let's use the sigmoid derivative as our surrogate gradient function, i.e., $$\Theta' = \sigma'$$. We will treat it as a known quantity even though it still has the $$'$$ symbol. $$\textcolor{brown}{\theta}$$ is just a constant and $$\textcolor{ForestGreen}{U[t]}$$ is known at time $$t$$ so we now know the left side of the equation. All that's left is $$\textcolor{ForestGreen}{U'[t]}$$ the derivative of the mebrane potential with respect to the input weight $$\textcolor{red}{W_{in}}$$.

To find the derivative of $$U[t]$$, let's remember its definition: $$\textcolor{ForestGreen}{U[t]} = \textcolor{blue}{\alpha} \textcolor{ForestGreen}{U[t-1]} + \textcolor{red}{W_{in}} \textcolor{purple}{X[t]}$$. Since $$U[t]$$ depends on $$W_in$$ and obviously $$\textcolor{red}{W_{in}} \textcolor{purple}{X[t]}$$ depends on $$W_{in}$$ as well, we get two parts to the derivative:


$$\textcolor{ForestGreen}{U'[t]} = (\textcolor{blue}{\alpha} \textcolor{ForestGreen}{U[t-1]})' + (\textcolor{red}{W_{in}} \textcolor{purple}{X[t]})'$$


Once again, $$\textcolor{blue}{\alpha}$$ is a constant, so the first term comes out to $$\textcolor{blue}{\alpha} \textcolor{ForestGreen}{U'[t-1]}$$. For the second term, obviously the derivative with respect to $$W_{in}$$ is simply $$(\textcolor{red}{W_{in}} \textcolor{purple}{X[t]})' = \textcolor{purple}{X[t]}$$

And thus the derivative of $$\textcolor{ForestGreen}{U[t]}$$ is:

\begin{equation}
\textcolor{ForestGreen}{U'[t]} = \textcolor{blue}{\alpha} \textcolor{ForestGreen}{U'[t-1]} + \textcolor{purple}{X[t]}
\label{eq:membrane_derivative_recursive}
\end{equation}


Okay that was straight-forward but now we have a weird recursion because the derivative of $$\textcolor{ForestGreen}{U[t]}$$ depends on the derivative of $$\textcolor{ForestGreen}{U[t-1]}$$. That one depends on $$\textcolor{ForestGreen}{U[t-2]}$$ and so on. The recursion ends when we reach the initial state, which is just a constant. It's much easier to work upwards from the initial state.

## Solving the recursion


The goal is to find the pattern in this recursion and then take its derivative. It will only take a couple of steps, but first let's set an initial state $$\textcolor{ForestGreen}{U[0]} = 0$$ and hence $$\textcolor{ForestGreen}{U'[0]} = 0$$. In this case, we know that 


$$\textcolor{ForestGreen}{U[1]} = \textcolor{blue}{\alpha} \textcolor{ForestGreen}{U[0]} + \textcolor{red}{W_{in}} \textcolor{purple}{X[0]}$$


$$ = \textcolor{blue}{\alpha} 0 + \textcolor{red}{W_{in}} \textcolor{purple}{X[1]} $$

$$ = \textcolor{red}{W_{in}} \textcolor{purple}{X[1]} $$


Going one step further, we find that: 


$$\textcolor{ForestGreen}{U[2]} = \textcolor{blue}{\alpha} \textcolor{ForestGreen}{U[1]} + \textcolor{red}{W_{in}} \textcolor{purple}{X[2]}$$


$$= \textcolor{blue}{\alpha} (\textcolor{red}{W_{in}} \textcolor{purple}{X[1]}) + \textcolor{red}{W_{in}} \textcolor{purple}{X[2]}$$


$$= \textcolor{blue}{\alpha} \textcolor{red}{W_{in}} \textcolor{purple}{X[1]} + \textcolor{red}{W_{in}} \textcolor{purple}{X[2]}$$


and another step:


$$\textcolor{ForestGreen}{U[3]} = \textcolor{blue}{\alpha} \textcolor{ForestGreen}{U[2]} + \textcolor{red}{W_{in}} \textcolor{purple}{X[3]}$$


$$ = \textcolor{blue}{\alpha} (\textcolor{blue}{\alpha} \textcolor{red}{W_{in}} \textcolor{purple}{X[1]} + \textcolor{red}{W_{in}} \textcolor{purple}{X[2]}) + \textcolor{red}{W_{in}} \textcolor{purple}{X[3]}$$


$$ = \textcolor{blue}{\alpha}^2 \textcolor{red}{W_{in}} \textcolor{purple}{X[1]} + \textcolor{blue}{\alpha} \textcolor{red}{W_{in}} \textcolor{purple}{X[2]} + \textcolor{red}{W_{in}} \textcolor{purple}{X[3]}$$


To make the pattern more obvious, let me add some exponents: 


$$\textcolor{ForestGreen}{U[3]} = \textcolor{blue}{\alpha}^2 \textcolor{red}{W_{in}} \textcolor{purple}{X[1]} + \textcolor{blue}{\alpha}^1 \textcolor{red}{W_{in}} \textcolor{purple}{X[2]} + \textcolor{blue}{\alpha}^0 \textcolor{red}{W_{in}} \textcolor{purple}{X[3]}$$


In general, we find that


$$\textcolor{ForestGreen}{U[t]} = \textcolor{blue}{\alpha}^{t-1} \textcolor{red}{W_{in}} \textcolor{purple}{X[1]} + \textcolor{blue}{\alpha}^{t-2} \textcolor{red}{W_{in}} \textcolor{purple}{X[2]} + ... + \textcolor{blue}{\alpha}^{t-t} \textcolor{red}{W_{in}} \textcolor{purple}{X[t]}$$

$$ = \sum_{i=1}^{t} \textcolor{blue}{\alpha}^{t-i} \textcolor{red}{W_{in}} \textcolor{purple}{X[i]}$$


Since the derivative of the sum is just the sum of the derivatives, we get the following equation:

\begin{equation}
\textcolor{ForestGreen}{U'[t]} = \sum_{i=1}^{t} \textcolor{blue}{\alpha}^{t-i} \textcolor{purple}{X[i]}
\label{eq:membrane_derivative_closed}
\end{equation}

## Putting it all together
Finally, we know the derivatives of all the parts. The loss $$\textcolor{olive}{E}$$, the spike $$\textcolor{orange}{S[t]}$$ and the membrane potential $$\textcolor{ForestGreen}{U[t]}$$. Let's assemble the final equation and then we will plug in the things we just derived. Our goal was to find $$\textcolor{olive}{E}'$$, the change change in loss with respect to the change in $$\textcolor{red}{W_{in}}$$. Let's follow the whole chain of equations again


$$\textcolor{olive}{E} = \textcolor{green}{\hat{y}} - \textcolor{magenta}{y}$$


$$\textcolor{olive}{E'} = \textcolor{green}{\hat{y}}'$$


$$\textcolor{green}{\hat{y}}' = \sum_t \textcolor{orange}{S'[t]}$$


$$\textcolor{orange}{S'[t]} = \Theta'(\textcolor{ForestGreen}{U[t]} - \textcolor{brown}{\theta}) \cdot \textcolor{ForestGreen}{U'[t]}$$


$$ \textcolor{ForestGreen}{U'[t]} = \sum_{i=1}^{t} \textcolor{blue}{\alpha}^{t-i} \textcolor{purple}{X[i]}$$


Great! Now we have the full chain of computations for BPTT. For an input connection, you could now compute its gradients by hand. Or almost I should say, because I dropped the reset term earlier. Let's add it back

## Adding back the reset term
WIP

## Dealing with recurrent weights
WIP